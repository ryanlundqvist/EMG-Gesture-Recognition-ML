import streamlit as st

st.title("ü§ñ Machine Learning | EMG | G29")
st.header("Introduction & Background", divider="rainbow")
st.markdown(
"""
What is an EMG (Electromyogram)?
- Bioelectrical signals generated by muscle cells when they are activated [1]. 
- Captured using electrodes placed on a skin surface. 
  - In regards to our dataset, The Myo Thalmic bracelet captures signals from the forearm. It does this by utilizing 8 EMG sensors and a bluetooth module. 
"""
)

st.markdown(
"""
What is EMG Gesture Classification?
- Process in which electric signals gendered by muscle contractions are monitored inorder to identify specific gestures [1]. 
"""
)

st.subheader("Literature Review", divider="blue")
st.markdown(
"""
Article: ‚ÄúHuman Hand Movement Classification based on EMG Signal using different Feature Extractor‚Äù
DOI: ‚Äã‚Äãhttps://dx.doi.org/10.13005/bpj/2835
- This paper focuses on applying certain feature selection techniques for classifiers in the medical sector [1].

"""
)
st.markdown(
"""
Article: ‚ÄúElectromyogram-Based Classification of Hand and Finger Gestures Using Artificial Neural Networks‚Äù
DOI: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8749583/
- This paper attempts to use time-domain features only to reduce the computational complexity instead of including frequency domain features. They also attempted to develop personalized classifiers for each person‚Äôs EMG data. They found that overall Artificial Neural Networks did best compared to SVMs, Random Forests, and Logistic Regression [2]. 
"""
)
st.markdown(
"""
Article: ‚ÄúGesture Classification in Electromyography Signals for Real-Time Prosthetic Hand Control Using a Convolutional Neural Network-Enhanced Channel Attention Model‚Äù
DOI: https://www.mdpi.com/2306-5354/10/11/1324
- This paper describes the approach of using a CNN-ECA gesture recognition framework, utilizing a combination of the CNN architecture and ECA module to better enhance the focus and capture ability of important features. It also shows techniques for reducing the influence of noise and improving signal steadiness [3].
"""
)
st.markdown(
"""
Article: ‚ÄúEMG-based online classification of gestures with recurrent neural networks‚Äù
DOI: https://www.sciencedirect.com/science/article/pii/S0167865519302089?casa_token=S5bUDUK0LzwAAAAA:4ITvJzw3veWQvbAnLJTPRQTyMjddRZSMujJ_Lw-bqfIBOvSjje_uXd69xGUXXIplcwjLH-KNbQ
- This paper involves using RNNs for online hand gesture classification with EMG signals, avoiding motion detection calibration. In the paper they compare FFNN, RNN, LSTM, and GRU models finding that they achieve similar accuracy but that LSTM/GRU are most efficient/faster to train [4].
"""
)

st.subheader("Dataset Description", divider="blue")
st.write("URL: https://archive.ics.uci.edu/dataset/481/emg+data+for+gestures")
st.write("Consists of gesture EMG recordings from a bluetooth MYO Thalmic bracelet. Time based data with raw EMG output, where each ‚Äúdatapoint‚Äù is a static hand gesture held for 3 seconds with a 3 second pause between different gestures. We can classify a total of 7 gestures. Each gesture is not uniformly represented in the dataset.")


st.header("Problem Definition", divider="rainbow")
st.subheader("Problem", divider="blue")
st.write("Electromyography (EMG) data, captured from muscle movements, is an important source of info which allows us to develop and create systems and technology that can properly interpret hand gestures. With proper interpretation, EMG data is useful in aiding the development of prosthetics and nuanced rehabilitation techniques. However, raw EMG data remains inherently variable and quite complicated, therefore making it quite difficult to accurately read and classify hand gestures.")
st.subheader("Motivation", divider="blue")
st.write("Properly reading and interpreting EMG data has massive implications in physical rehabilitation and prosthetic developments [2], as it would allow us to better mimic natural and intuitive control of limbs, allowing those with amputations and disabilities greater independence and mobility in their lives. In fact, functional prosthetics can improve mental well-being and reduce muscle-atropphy [3]. In this project, we will be exploring the use of ML to enhance the accuracy and reliability of hand gesture recognition, providing a foundation for real-world applications by tackling the inherent variability of EMG data.")



st.header("Methods", divider="rainbow")
st.subheader("Data Preprocessing", divider="blue")
st.write("So first, we began by cleaning the dataset by removing unmarked data (class 0) to ensure relevance. Features and labels were separated, and outliers were eliminated using the Z-score method with a threshold of 3 to maintain data integrity. The data was then standardized using `StandardScaler` to normalize it. To capture temporal patterns in the EMG signals, we segmented the data into overlapping windows (window size of 100 with 50% overlap). Then from each window, we extracted statistical features such as standard deviation, maximum, minimum, RMS, absolute mean, and zero crossings. Finally, we split the processed data into training and testing sets which we used for the training/classification.")
st.code("""

def processAllSubs(root_dir = "EMG_data_for_gestures-master", window_size=100, overlap=0.5):
    processed_subs = {}
    for sub_id in range(1, 37):
        print(f"Doing subject {sub_id}...")
        print("======================")
        curr = fo.analyze_single_subject(sub_id)

        print("Cleaning data...")
        X, y = cleanDataAndSepFeat(curr)
        X_standardized = X
        #X_standardized = standardize(X)
        #print("Size after cleaning", X_standardized.shape)
        #print("--------------")

        print("Removing outliers...")
        X_no_outliers = removeOutliers(X_standardized, threshold=3)
        #print("Size after outliers", X_no_outliers.shape)
        #print("--------------")
    
        #maybe include reduce noise here
        
        print("Creating windows...")
        windows, window_labels = createWindows(X_no_outliers, y, window_size, overlap)
        #print("Size after windows: ", len(windows))
        #print("--------------")
    
        
        print("Extracting features...")
        X_features = extractFeaturesForWindows(windows)
        print("Size of features on subject", sub_id, ": ", X_features.shape)
        print("--------------")

        processed_subs[sub_id] = {'features': X_features, 'labels': window_labels}
        
        
        print("======================")
    return processed_subs
def single_model_data(processed_subs):
    all_feats = []
    all_labels = []
    #feature_names = X_train.columns
    for sub_data in processed_subs.values():
        all_feats.append(sub_data['features'])
        all_labels.append(sub_data['labels'])
    X = pd.concat(all_feats)
    y = np.concatenate(all_labels)
    
    X_train, X_test, y_train, y_test = splitData(X, y)
    #print(X_train)
    feat_names = X_train.columns
    print(feat_names)

    #scale data (would scale it in processing subjects if we did LOSO)
    #X_train_scaled = StandardScaler().fit_transform(X_train)
    X_train_scaled = pd.DataFrame(StandardScaler().fit_transform(X_train), columns = feat_names)
    X_test_scaled = pd.DataFrame(StandardScaler().fit_transform(X_test), columns = feat_names)
    #X_test_scaled = StandardScaler().fit_transform(X_test)
    return X_train_scaled, X_test_scaled, y_train, y_test
#processed_subjects = processAllsubs()

""", language="python", line_numbers=True)

st.subheader("ML Methods Implemented", divider="blue")
st.write("---------------------------------------------------------- Random Forest ----------------------------------------------------------")
st.code("""
def train_rf_classifier(X_train, y_train):

    rf_classifier = RandomForestClassifier(
        n_estimators=1000,
        random_state=42,
        max_depth=None,
        min_samples_split=20,
        min_samples_leaf=10
    )

    rf_classifier.fit(X_train, y_train)

    return rf_classifier
        """, language="python", line_numbers=True)
st.write("For our first model, we chose to use a **Random Forest** classifier for gesture classification. This is an ensemble method and was (in part) chosen because of its robustness against overfitting, ability to handle high-dimensional feature spaces (like EMG data), and effectiveness in capturing complex patterns within the EMG data. The model was configured with 1000 decision trees, no maximum depth, and specified minimum samples for splits and leaves to enhance generalization. Another thing to note is that a Random Forest also provides feature importance metrics, allowing us to identify and (potentially) prioritize the most influential features in the classification process. We think all of these characteristics make it generally well-suited for classifying gestures from EMG signals.")

st.write("---------------------------------------------------------- GMM ----------------------------------------------------------")
st.code("""
def train_rf_classifier(X_train, y_train):

    rf_classifier = RandomForestClassifier(
        n_estimators=1000,
        random_state=42,
        max_depth=None,
        min_samples_split=20,
        min_samples_leaf=10
    )

    rf_classifier.fit(X_train, y_train)

    return rf_classifier
        """, language="python", line_numbers=True)
st.write("For our first model, we chose to use a **Random Forest** classifier for gesture classification. This is an ensemble method and was (in part) chosen because of its robustness against overfitting, ability to handle high-dimensional feature spaces (like EMG data), and effectiveness in capturing complex patterns within the EMG data. The model was configured with 1000 decision trees, no maximum depth, and specified minimum samples for splits and leaves to enhance generalization. Another thing to note is that a Random Forest also provides feature importance metrics, allowing us to identify and (potentially) prioritize the most influential features in the classification process. We think all of these characteristics make it generally well-suited for classifying gestures from EMG signals.")

st.write("---------------------------------------------------------- Neural Network ----------------------------------------------------------")
st.code("""
def train_rf_classifier(X_train, y_train):

    rf_classifier = RandomForestClassifier(
        n_estimators=1000,
        random_state=42,
        max_depth=None,
        min_samples_split=20,
        min_samples_leaf=10
    )

    rf_classifier.fit(X_train, y_train)

    return rf_classifier
        """, language="python", line_numbers=True)
st.write("For our first model, we chose to use a **Random Forest** classifier for gesture classification. This is an ensemble method and was (in part) chosen because of its robustness against overfitting, ability to handle high-dimensional feature spaces (like EMG data), and effectiveness in capturing complex patterns within the EMG data. The model was configured with 1000 decision trees, no maximum depth, and specified minimum samples for splits and leaves to enhance generalization. Another thing to note is that a Random Forest also provides feature importance metrics, allowing us to identify and (potentially) prioritize the most influential features in the classification process. We think all of these characteristics make it generally well-suited for classifying gestures from EMG signals.")


st.header("Results & Discussion", divider="rainbow")
st.subheader("Visualization (Confusion Matrix)", divider="blue")
st.image("ConfMatrixRandomForest_EMG_4641.png")
st.write("We can see that the diagonal dominance in the confusion matrix above indicates that the model was able to correctly classify most samples in each seperate class. We are also able to see how the model doesn't perform as well with classes 3 and 6 (high values not on the diagonal for those class intersections), which could suggest some class imbalance. It is also more apparent because it seems like class 7 did not have nearly as many samples as the other classes, further signifying a potential class imbalance. To fix this, one could resample the data or use synthetic data generation, which would improve the model's performance on the classes with less samples.")
st.subheader("Quantitative Metrics (Accuracy, Precision, Recall)", divider="blue")
st.markdown("""
- **Accuracy:** With the model achieving an accuracy of ~86%, we can see that it is able to successfully classify a significant majority of our data, supporting the fact that the random forest classifier performed relatively well overall. However, it is important to note that this accuracy, while good, doesn't account for the misclassification jumps in specific classes that could be significant in specific use cases.
- **Precision:** Our precision gives us an indicator of the proportion of correctly predicted gestures out of all gestures that the model predicted for each class. Overall, we can see that we obtained a weighted average precision of 0.87, which is fairly good. This precision does vary by class however, with a minimum of 0.81 and a maximum of 1.0. 
- **Recall:** The recall score helps us understand the proportion of actual gestures that were correctly predicted by the model. We were able to obtain a weighted average recall of 0.86, which is also fairly good. Unfortunately, this varies all the way down to a recall of 0.26 for class 7.0. 

With an F1 score of ~0.86, we are able to see that the model is able to act precisely with decent recall. The high precision shown in most classes compared to a slightly lower recall signify that the model is good at picking up negatives (e.g doesn't classify many false positives), but it is missing some true positives at times as well.
""")
st.subheader("Analysis of Algorithm: Random Forest", divider="blue")
st.markdown("""
For making classifications, we use the **random forest** method with scikit-learn, which is an ensemble method of multiple decision trees, combining their results to make more accurate classifications and control overfitting. 

A **decision tree** is a structure where classification flows from the root to a leaf node, where each intermediary node represents a decision based on a feature, each branch represents the subsequent outcome and choices based on the parent decision made, and each leaf node represents a final classification. When splitting the data based on a feature, the decision tree uses the metric of **Gini Impurity** to make each subset of data as pure as possible and improve classification accuracy (by reducing uncertainty about what a classification should be based on feature information). 

When $D$ is the dataset, $C$ is the number of classes, and $p_i$ is the probability that a randomly selected point in $D$ belongs to class $i$, Gini Impurity can be calculated as follows:

$$Gini(D) = 1 - \sum_{i=1}^{C} (p_i)^2$$

When using the random forest method, we randomly select subsets of the training data to create diverse datasets, and for each of these subsets we build a decision tree by finding the best feature to split the data at each node. For the actual broader classification, each tree votes for a class assignment and the majority becomes the final prediction. T

When $\hat{y}$ is the prediction in a Random Forest, and $DT_i$ is the classification made by the $i$-th Decision Tree, this voting can be formalized with $n$ many trees as follows:

$$\hat{y} = mode\{ DT_1(x), DT_2(x), \ldots, DT_n(x) \}$$

This technique is a solid fit for our problem space, because it is relatively good at robustly handling datasets that are large with high dimensionality (such as our EMG dataset) and the use of multiple decision trees "voting" can help cancel out imperfections caused by signal noise, which is difficult to fully remove from sensor data datasets, such as the dataset we use. Random forests are also convenient in that they lend themselves to being more interpretable than some other methods, since you can see how a classification was made in the decision trees that voted and can directly observe and visualize what features were important for that classification.

For our implementation, we use a Random Forest of 1000 decision trees, with a starting seed of 42, no max depth (allowing for more purity), a minimum samples required to split of 20 (reducing overfitting for very small sample numbers), and a minimum number of samples in a leaf node of 10. Our model has decently solid performance, with an accuracy score of about 0.8634. Additionally, there is a decent balance between precision and recall, with an f1 score of about 0.8621. However as discussed in the "Quantitative Metrics" section, there is still room to improve. The most important features were derived from channel 7 of the signal data.
            """)





st.subheader("Analysis of Algorithm: GMM", divider="blue")
st.markdown("""
For making classifications, we use the **random forest** method with scikit-learn, which is an ensemble method of multiple decision trees, combining their results to make more accurate classifications and control overfitting. 

A **decision tree** is a structure where classification flows from the root to a leaf node, where each intermediary node represents a decision based on a feature, each branch represents the subsequent outcome and choices based on the parent decision made, and each leaf node represents a final classification. When splitting the data based on a feature, the decision tree uses the metric of **Gini Impurity** to make each subset of data as pure as possible and improve classification accuracy (by reducing uncertainty about what a classification should be based on feature information). 

When $D$ is the dataset, $C$ is the number of classes, and $p_i$ is the probability that a randomly selected point in $D$ belongs to class $i$, Gini Impurity can be calculated as follows:

$$Gini(D) = 1 - \sum_{i=1}^{C} (p_i)^2$$

When using the random forest method, we randomly select subsets of the training data to create diverse datasets, and for each of these subsets we build a decision tree by finding the best feature to split the data at each node. For the actual broader classification, each tree votes for a class assignment and the majority becomes the final prediction. T

When $\hat{y}$ is the prediction in a Random Forest, and $DT_i$ is the classification made by the $i$-th Decision Tree, this voting can be formalized with $n$ many trees as follows:

$$\hat{y} = mode\{ DT_1(x), DT_2(x), \ldots, DT_n(x) \}$$

This technique is a solid fit for our problem space, because it is relatively good at robustly handling datasets that are large with high dimensionality (such as our EMG dataset) and the use of multiple decision trees "voting" can help cancel out imperfections caused by signal noise, which is difficult to fully remove from sensor data datasets, such as the dataset we use. Random forests are also convenient in that they lend themselves to being more interpretable than some other methods, since you can see how a classification was made in the decision trees that voted and can directly observe and visualize what features were important for that classification.

For our implementation, we use a Random Forest of 1000 decision trees, with a starting seed of 42, no max depth (allowing for more purity), a minimum samples required to split of 20 (reducing overfitting for very small sample numbers), and a minimum number of samples in a leaf node of 10. Our model has decently solid performance, with an accuracy score of about 0.8634. Additionally, there is a decent balance between precision and recall, with an f1 score of about 0.8621. However as discussed in the "Quantitative Metrics" section, there is still room to improve. The most important features were derived from channel 7 of the signal data.
            """)





st.subheader("Analysis of Algorithm: Neural Network", divider="blue")
st.markdown(""" 

Our neural network is a simple model for classification with two hidden layers using ReLU activation. It includes 50% and 30% dropout to prevent overfitting and trains with a learning rate of 0.001 using the Adam optimizer. We also track its performance with accuracy, F1-score, and a confusion matrix, plus plot its loss and accuracy over the training process.

The accuracy score is 65.8%, which indicates moderate performance on the test data.
The weighted average F1-score of 0.66 aligns with the accuracy, showing consistency accross classes.

We can see that the loss goes down in the first couple epochs then starts increasing, this issue might be that our model is too complex and that potential issues like learning rate, overfitting, or exploding gradients might exist.

""")





st.subheader("Next Steps", divider="blue")
st.markdown("""
Moving forward, we aim to both refine our methods for data preprocessing and attempt to improve our classification accuracy by attempting a variety of strategies. For instance, we are considering:  

- Hyperparameter tuning
- Data augmentation (adding noise or time-warping for robustness)
- Feature engineering
- Try one vs rest classification

Finally, we would like to test all three of our models with variable real-world data (if we get the chance), by using our own sensors to test how well the models respond to the change that comes from a new data source. 
""")

st.header("References", divider="rainbow")
st.write("[1]Swati Shilaskar, Shripad Bhatlawande, Ranveer Chavare, Aditya Ingale, R. Joshi, and Aditya Vaishale, ‚ÄúHuman Hand Movement Classification based on EMG Signal using different Feature Extractor,‚Äù Biomedical and Pharmacology Journal, vol. 17, no. 1, pp. 71‚Äì82, Mar. 2024, Available: https://biomedpharmajournal.org/vol17no1/human-hand-movement-classification-based-on-emg-signal-using-different-feature-extractor/")
st.write("[2]K. H. Lee, J. Y. Min, and S. Byun, ‚ÄúElectromyogram-Based Classification of Hand and Finger Gestures Using Artificial Neural Networks,‚Äù Sensors, vol. 22, no. 1, p. 225, Dec. 2021, doi: https://doi.org/10.3390/s22010225.")
st.write("[3]G. Yu, Z. Deng, Z. Bao, Y. Zhang, and B. He, ‚ÄúGesture Classification in Electromyography Signals for Real-Time Prosthetic Hand Control Using a Convolutional Neural Network-Enhanced Channel Attention Model,‚Äù Bioengineering, vol. 10, no. 11, pp. 1324‚Äì1324, Nov. 2023, doi: https://doi.org/10.3390/bioengineering10111324.")
st.write("[4]M. Sim√£o, P. Neto, and O. Gibaru, ‚ÄúEMG-based online classification of gestures with recurrent neural networks,‚Äù Pattern Recognition Letters, vol. 128, pp. 45‚Äì51, Dec. 2019, doi: https://doi.org/10.1016/j.patrec.2019.07.021. ‚Äå")


st.header("Gantt Chart", divider="rainbow")
st.image("4641_gantt.png")
st.header("Contributions", divider="rainbow")
st.image("contributions.png")


